{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# First Model"
      ],
      "metadata": {
        "id": "W1Mn_Z0g69Ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9HLjMnQj-g9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RICE Pest Multi Classification Model. Using Muiltiple Features From the Dataset, the model can make predictions on rice to Identify If it has pests and what pest it is likely to be.**"
      ],
      "metadata": {
        "id": "m2q-RDyO-mKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uSPh5nBLa61",
        "outputId": "9b4fad73-f5a0-436f-f985-636fc7f87249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Pest Value', 'MaxT', 'MinT', 'RH1(%)', 'RH2(%)', 'RF(mm)', 'WS(kmph)',\n",
            "       'SSH(hrs)', 'EVP(mm)', 'PEST NAME'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import sys, numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from numpy import argmax\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df= pd.read_csv('https://raw.githubusercontent.com/Russoremy/Datasets/main/RICE.csv')\n",
        "\n",
        "df.drop(['Observation Year','Standard Week','Collection Type','Location'], axis=1, inplace=True)\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5a3s_QaQ1zFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df.drop('PEST NAME', axis=1)\n",
        "y = df['PEST NAME']"
      ],
      "metadata": {
        "id": "4w7Mw8aO1gyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "# Split Data we are using a different data set than the one Dr. Lee used and it contains no images\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# Normalize numerical features using MinMaxScaler\n",
        "numerical_features = ['Pest Value', 'MaxT', 'MinT', 'RH1(%)', 'RH2(%)', 'RF(mm)', 'WS(kmph)', 'SSH(hrs)', 'EVP(mm)']\n",
        "scaler = MinMaxScaler()\n",
        "X_train_numerical = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_numerical = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# Concatenate categorical and numerical features\n",
        "X_train = np.concatenate([X_train_numerical], axis=1)\n",
        "X_test = np.concatenate([X_test_numerical], axis=1)\n",
        "\n",
        "# Target Encoding\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the target variable\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "# Reshape target variable to match the input shape\n",
        "y_train = y_train_encoded.reshape(-1, 1)\n",
        "y_test = y_test_encoded.reshape(-1, 1)\n",
        "\n",
        "relu = lambda x: (x >= 0) * x\n",
        "relu2deriv = lambda x: x >= 0\n",
        "batch_size = 100\n",
        "alpha, iterations, hidden_size, input_size = (0.001, 300, 40, X_train.shape[1])\n",
        "\n",
        "weights_0_1 = 0.2 * np.random.random((input_size, hidden_size)) - 0.1\n",
        "weights_1_2 = 0.2 * np.random.random((hidden_size, y_train.shape[1])) - 0.1\n",
        "\n",
        "for j in range(iterations):\n",
        "    error, correct_cnt = (0.0, 0)\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        layer_0 = X_train[i:i+1]\n",
        "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
        "        layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "        error += np.sum((y_train[i:i+1] - layer_2) ** 2)\n",
        "        correct_cnt += int(np.argmax(layer_2) == np.argmax(y_train[i:i+1]))\n",
        "\n",
        "        layer_2_delta = (y_train[i:i+1] - layer_2)\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
        "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "    sys.stdout.write(\"\\r I:\" + str(j) +\n",
        "                     \" Train-Err:\" + str(error / float(len(X_train)))[0:5] +\\\n",
        "                     \" Train-Acc:\" + str(correct_cnt / float(len(X_train))))\n",
        "\n",
        "\n",
        "if(j % 10 == 0 or j == iterations-1):\n",
        "    error, correct_cnt = (0.0, 0)\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "\n",
        "        layer_0 = X_test[i:i+1]\n",
        "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "\n",
        "        error += np.sum((y_test[i:i+1] - layer_2) ** 2)\n",
        "        correct_cnt += int(np.argmax(layer_2) == \\\n",
        "                                        np.argmax(y_test[i:i+1]))\n",
        "    sys.stdout.write(\" Test-Err:\" + str(error/float(len(X_test)))[0:5] +\\\n",
        "                     \" Test-Acc:\" + str(correct_cnt/float(len(X_test))) + \"\\n\")\n",
        "    print()\n",
        "\n",
        "    #The part I am stuck in and slighty confused with this type of example you showed is how I can tune it an reduce the error.\n",
        "    #I also believe the keras models are more simple and efficient but this allow for more customization for tunning. Which one is better practice?\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag1GqYoi1isT",
        "outputId": "102c9242-fc2c-4b19-f44d-39b99c6f401d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I:299 Train-Err:10.27 Train-Acc:1.0 Test-Err:10.24 Test-Acc:1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Model\n"
      ],
      "metadata": {
        "id": "QjDplTB37_6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
        "from numpy import argmax\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df= pd.read_csv('https://raw.githubusercontent.com/Russoremy/Datasets/main/RICE.csv')\n",
        "\n",
        "df.drop(['Observation Year','Standard Week','Collection Type','Location'], axis=1, inplace=True)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yNaV7aK8FeX",
        "outputId": "00013324-0831-4965-84c7-8f58a12709bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Pest Value', 'MaxT', 'MinT', 'RH1(%)', 'RH2(%)', 'RF(mm)', 'WS(kmph)',\n",
            "       'SSH(hrs)', 'EVP(mm)', 'PEST NAME'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "cb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='model-{epoch:02d}-{val_accuracy:.2f}.hdf5',\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce learning rate if no improvement in val_loss\n",
        "cb_reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    factor=0.1,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    min_lr=0.00001\n",
        ")\n",
        "\n",
        "# Stop training if no improvement in val_accuracy\n",
        "cb_earlystop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    min_delta=0.001,\n",
        "    patience=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save training history to CSV file\n",
        "cb_csvlogger = tf.keras.callbacks.CSVLogger(\n",
        "    filename='training_log.csv',\n",
        "    separator=',',\n",
        "    append=False\n",
        ")"
      ],
      "metadata": {
        "id": "hwgBmumIIUuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data we are using a different data set than the one Dr. Lee used and it contains no images\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# Normalize numerical features using MinMaxScaler\n",
        "numerical_features = ['Pest Value', 'MaxT', 'MinT', 'RH1(%)', 'RH2(%)', 'RF(mm)', 'WS(kmph)', 'SSH(hrs)', 'EVP(mm)']\n",
        "scaler = MinMaxScaler()\n",
        "X_train_numerical = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_numerical = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# Concatenate categorical and numerical features\n",
        "X_train = np.concatenate([X_train_numerical], axis=1)\n",
        "X_test = np.concatenate([X_test_numerical], axis=1)\n",
        "\n",
        "# Target Encoding\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the target variable\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "# Reshape target variable to match the input shape\n",
        "y_train = y_train_encoded.reshape(-1, 1)\n",
        "y_test = y_test_encoded.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "WtXHJ2vv8eE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine the number of input features\n",
        "n_features = X_train.shape[1]\n",
        "n_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU6wqAyL8m5u",
        "outputId": "fcf926d2-2793-4797-c67a-ec42ae6c791d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(1, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "Ul2GCBmV8qRF",
        "outputId": "4ae825a8-48d7-4436-f3f8-abd3a30c9945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1300/1300 [==============================] - 6s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0479\n",
            "Epoch 2/100\n",
            "1300/1300 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0479\n",
            "Epoch 3/100\n",
            "1300/1300 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0479\n",
            "Epoch 4/100\n",
            "1300/1300 [==============================] - 2s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0479\n",
            "Epoch 5/100\n",
            "1300/1300 [==============================] - 3s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0479\n",
            "Epoch 6/100\n",
            " 591/1300 [============>.................] - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.0467"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-4f08ffabc263>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1682\u001b[0m                             \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[0;32m-> 1684\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"begin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_begin_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;34m\"\"\"Helper function for `on_*_batch_begin` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"on_{mode}_batch_begin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;34m\"\"\"Helper function for `on_*_batch_*` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Accuracy: %.3f' % acc)\n",
        "# make a prediction\n",
        "row = [1, 29.6,\t11.3,\t80, 32.7, 1, 0, 0, 0 ]\n",
        "yhat = model.predict([row])\n",
        "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ifwG5sh-AhS",
        "outputId": "64a5825e-c44b-4926-b713-7db2687859f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.050\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "Predicted: [[1.]] (class=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search"
      ],
      "metadata": {
        "id": "P4OOYRvDC-h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def create_model(neurons_1=1, neurons_2=1):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons_1, input_dim=9, kernel_initializer='uniform', activation='linear'))\n",
        "    model.add(Dense(neurons_2, kernel_initializer='uniform', activation='linear'))\n",
        "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "neurons_1 = [1, 5, 10, 15, 20, 25, 30, 50, 100]\n",
        "neurons_2 = [1, 5, 10, 15, 20, 25, 30, 50, 100]\n",
        "param_grid = dict(neurons_1=neurons_1, neurons_2=neurons_2)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train, callbacks=[cb_earlystop], validation_split=0.2)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "id": "nA5bh_ZWDDx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc26b0c8-3d13-4f6c-f855-1d24c9b965ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: early stopping\n",
            "Best: 0.047923 using {'neurons_1': 1, 'neurons_2': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKlUCPBzGX6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}